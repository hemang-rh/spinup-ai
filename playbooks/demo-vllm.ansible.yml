- name: Demo VLLM Model Serving
  hosts: localhost
  vars:
    repo_dir: "{{ lookup('env', 'REPO_DIR') | default(playbook_dir + '/..', true) }}"
  tasks:
    - name: Create demo-vllm namespace
      shell: "oc apply -f {{ repo_dir }}/demo/vllm-modelserving/demo-vllm-ns.yaml"
      register: vllm_modelserving_apply
      retries: 3
      delay: 10
      until: vllm_modelserving_apply.rc == 0

    - name: Validate that demo-vllm namespace got created
      shell: "oc get namespace demo-vllm --no-headers"
      register: demo_vllm_ns
      retries: 5
      delay: 5
      until: demo_vllm_ns.rc == 0
      changed_when: false

    - name: Create servingruntime and in demo-vllm namespace
      shell: |
        oc project demo-vllm
        oc apply -f {{ repo_dir }}/demo/vllm-modelserving/vllm-servingruntime.yaml
        oc apply -f {{ repo_dir }}/demo/vllm-modelserving/inferenceservice.yaml
      register: vllm_servingruntime_apply
      retries: 3
      delay: 10
      until: vllm_servingruntime_apply.rc == 0

    - name: Wait for pod starting with granite-model to be running and ready in demo-vllm namespace
      shell: |
        oc get pods -n demo-vllm -o json | jq -r '.items[] | select(.metadata.name | test("^granite-model")) | select(.status.phase=="Running") | select(.status.containerStatuses[0].ready==true) | .metadata.name'
      register: granite_model_pod_ready
      retries: 20
      delay: 15
      until: granite_model_pod_ready.stdout != ""
      changed_when: false

    - name: Print granite-model pod status
      debug:
        msg: "Pod starting with granite-model is running and ready: {{ granite_model_pod_ready.stdout }}"
      when: granite_model_pod_ready.stdout != ""
